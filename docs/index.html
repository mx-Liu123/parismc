<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PARIS Monte Carlo Sampler</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }
        .header {
            text-align: center;
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px solid #eee;
        }
        .features {
            display: grid;
            gap: 20px;
            margin: 30px 0;
        }
        .feature-card {
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 8px;
            background: #f9f9f9;
        }
        .code-block {
            background: #f5f5f5;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            margin: 15px 0;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        .install-section {
            background: #e8f4f8;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }
        .nav-links {
            text-align: center;
            margin: 30px 0;
        }
        .nav-links a {
            display: inline-block;
            margin: 0 15px;
            padding: 10px 20px;
            background: #007acc;
            color: white;
            text-decoration: none;
            border-radius: 5px;
        }
        .nav-links a:hover {
            background: #005999;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>PARIS Monte Carlo Sampler</h1>
        <p><strong>An efficient adaptive importance sampler for high-dimensional multi-modal Bayesian inference</strong></p>
    </div>

    <div class="nav-links">
        <a href="#algorithm">Algorithm</a>
        <a href="#installation">Installation</a>
        <a href="#quick-start">Quick Start</a>
        <a href="#performance">Performance</a>
        <a href="#api-reference">API Reference</a>
        <a href="#examples">Examples</a>
        <a href="https://github.com/mx-Liu123/parismc">GitHub</a>
    </div>

    <section>
        <h2>About PARIS</h2>
        <p>PARIS (<strong>Parallel Adaptive Reweighting Importance Sampling</strong>) combines global exploration with local adaptation to tackle complex posteriors. The workflow is simple:</p>
        
        <ol>
            <li><strong>Global Initialization</strong>: Start with a space-filling design (e.g. Latin Hypercube Sampling) to seed promising regions.</li>
            <li><strong>Adaptive Proposals</strong>: Each seed runs its own importance sampling process, where the proposal is a Gaussian mixture centered on past weighted samples with covariance estimated from the local sample set.</li>
            <li><strong>Dynamic Reweighting</strong>: All samples are reweighted against the evolving proposal mixture, ensuring unbiased estimates and self-correcting any early overweights.</li>
            <li><strong>Mode Clustering</strong>: Parallel processes that converge to the same region are merged to avoid redundancy, while distinct modes are preserved.</li>
            <li><strong>Posterior & Evidence</strong>: The collected weighted samples directly reconstruct the posterior and yield accurate Bayesian evidence estimates.</li>
        </ol>
    </section>

    <section id="algorithm">
        <h2>Algorithm</h2>
        
        <h3>PARIS Algorithm (Simplified Implementation)</h3>
        
        <h4>Initialization (T=1)</h4>
        <ol>
            <li><strong>Distribute N<sub>LHS</sub> LHS points</strong> across all prior regions</li>
            <li><strong>Select N<sub>seed</sub> points</strong> with highest posterior as process initializations {x<sub>seed</sub><sup>(j)</sup>}<sub>j=1</sub><sup>N<sub>proc</sub></sup></li>
            <li><strong>For each process</strong> x<sub>seed</sub><sup>(j)</sup> (j = 1, ..., N<sub>proc</sub>):
                <ul>
                    <li>3.1) Predefine proposal covariance Œ£<sub>1</sub><sup>(j)</sup> = Œ£<sub>init</sub></li>
                    <li>3.2) Generate first sample: x<sub>1</sub><sup>(j)</sup> ~ N(x | x<sub>seed</sub><sup>(j)</sup>, Œ£<sub>1</sub><sup>(j)</sup>)</li>
                    <li>3.3) Initialize importance weight: w<sub>1</sub><sup>(j)</sup> = P(x<sub>1</sub><sup>(j)</sup>) / q<sub>1</sub><sup>(j)</sup>(x<sub>1</sub><sup>(j)</sup>)</li>
                </ul>
            </li>
        </ol>

        <h4>General Iteration (T > 1)</h4>
        <ol>
            <li><strong>For each process</strong> j = 1, ..., N<sub>proc</sub>:
                <ul>
                    <li>1.1) <strong>Compute weighted mean and covariance</strong>:</li>
                </ul>
                <div class="code-block">Œº<sub>T</sub><sup>(j)</sup> = Œ£<sub>t=1</sub><sup>T-1</sup> w<sub>t</sub><sup>(j)</sup> x<sub>t</sub><sup>(j)</sup> / Œ£<sub>t=1</sub><sup>T-1</sup> w<sub>t</sub><sup>(j)</sup>

Œ£<sub>T</sub><sup>(j)</sup> = Œ£<sub>t=1</sub><sup>T-1</sup> w<sub>t</sub><sup>(j)</sup> (x<sub>t</sub><sup>(j)</sup> - Œº<sub>T</sub><sup>(j)</sup>)(x<sub>t</sub><sup>(j)</sup> - Œº<sub>T</sub><sup>(j)</sup>)<sup>T</sup> / Œ£<sub>t=1</sub><sup>T-1</sup> w<sub>t</sub><sup>(j)</sup></div>
                <ul>
                    <li>1.2) <strong>Define proposal</strong> as weighted Gaussian mixture:</li>
                </ul>
                <div class="code-block">q<sub>T</sub><sup>(j)</sup>(x) = Œ£<sub>t=1</sub><sup>T-1</sup> w<sub>t</sub><sup>(j)</sup> N(x | x<sub>t</sub><sup>(j)</sup>, Œ£<sub>T</sub><sup>(j)</sup>) / Œ£<sub>t=1</sub><sup>T-1</sup> w<sub>t</sub><sup>(j)</sup></div>
                <ul>
                    <li>1.3) <strong>Choose component</strong> N(x | y<sub>T</sub><sup>(j)</sup>, Œ£<sub>T</sub><sup>(j)</sup>) from q<sub>T</sub><sup>(j)</sup>(x)</li>
                    <li>1.4) <strong>Draw new sample</strong> x<sub>T</sub><sup>(j)</sup> from N(x | y<sub>T</sub><sup>(j)</sup>, Œ£<sub>T</sub><sup>(j)</sup>)</li>
                    <li>1.5) <strong>Update past importance weights</strong> for t = 1, ..., T:</li>
                </ul>
                <div class="code-block">w<sub>t</sub><sup>(j)</sup> ‚Üê P(x<sub>t</sub><sup>(j)</sup>) / (1/T √ó Œ£<sub>t'=1</sub><sup>T</sup> q<sub>t'</sub><sup>(j)</sup>(x<sub>t</sub><sup>(j)</sup>))</div>
                <ul>
                    <li>1.6) <strong>Save weighted samples</strong> and proposals at iteration T</li>
                </ul>
            </li>
            <li><strong>Process Interaction</strong>:
                <ul>
                    <li>2.1) Initialize all processes as unvisited</li>
                    <li>2.2) <strong>For each unvisited process</strong> mean Œº<sub>T</sub><sup>(j)</sup>, find neighbors Œº<sub>T</sub><sup>(j')</sup> satisfying:</li>
                </ul>
                <div class="code-block">R<sub>j'‚Üíj</sub> = ‚àö((Œº<sub>T</sub><sup>(j')</sup> - Œº<sub>T</sub><sup>(j)</sup>)<sup>T</sup> [Œ£<sub>T</sub><sup>(j)</sup>]<sup>-1</sup> (Œº<sub>T</sub><sup>(j')</sup> - Œº<sub>T</sub><sup>(j)</sup>)) ‚â§ R<sub>m</sub></div>
                <ul>
                    <li>2.3) <strong>Form clusters</strong> of j and neighbors, mark as visited</li>
                    <li>2.4) <strong>In each cluster</strong>, retain process with highest posterior max P(x<sub>T</sub><sup>(j)</sup>), terminate others</li>
                </ul>
            </li>
        </ol>

        <h4>Advanced Implementation Notes</h4>
        <ul>
            <li>Covariance matrices updated every <strong>Œ≥ iterations</strong> (practical version)</li>
            <li>Truncation uses latest <strong>Œ± samples</strong> for weighting calculations</li>
            <li><strong>Beta correction</strong> applied for boundary effects when >10% samples fall outside [0,1]<sup>d</sup></li>
            <li><strong>OAS shrinkage</strong> applied for robust covariance estimation with small samples</li>
        </ul>
    </section>

    <section>
        <h2>Key Features</h2>
        <div class="features">
            <div class="feature-card">
                <h3>üéØ Adaptive Proposals per Seed</h3>
                <p>Each process maintains its own proposal, evolving a local Gaussian mixture that adapts to past samples.</p>
            </div>
            <div class="feature-card">
                <h3>‚öñÔ∏è Auto-balanced Exploration</h3>
                <p>High-weight discoveries automatically attract more samples, while overweights self-correct over time.</p>
            </div>
            <div class="feature-card">
                <h3>üìä Accurate Evidence Estimation</h3>
                <p>Bayesian evidence is computed directly from importance weights, no extra machinery needed.</p>
            </div>
            <div class="feature-card">
                <h3>üöÄ Parallel Mode Discovery</h3>
                <p>Multiple seeds explore independently, merging only when they converge to the same mode.</p>
            </div>
        </div>
    </section>

    <section id="installation">
        <h2>Installation</h2>
        <div class="install-section">
            <h3>From PyPI (when available)</h3>
            <div class="code-block">pip install parismc</div>
            
            <h3>From Source</h3>
            <div class="code-block">git clone https://github.com/mx-Liu123/parismc.git
cd parismc
pip install -e .</div>
        </div>
    </section>

    <section id="quick-start">
        <h2>Quick Start</h2>
        <div class="code-block">import numpy as np
from parismc import Sampler, SamplerConfig

# Define your log-likelihood function
def log_likelihood(x):
    """Example: multivariate Gaussian log-likelihood"""
    return -0.5 * np.sum(x**2, axis=1)

# Create sampler configuration
config = SamplerConfig(
    alpha=1000,
    boundary_limiting=True,
    use_pool=False  # Set to True for multiprocessing
)

# Initialize sampler
ndim = 2
n_seed = 5
init_cov_list = [np.eye(ndim) * 0.1] * n_seed

sampler = Sampler(
    ndim=ndim,
    n_seed=n_seed,
    log_density_func=log_likelihood,
    init_cov_list=init_cov_list,
    config=config
)

# Prepare initial samples
sampler.prepare_lhs_samples(lhs_num=1000, batch_size=100)

# Run sampling
sampler.run_sampling(num_iterations=500, savepath='./results')

# Get results
samples, weights = sampler.get_samples_with_weights(flatten=True)</div>
    </section>

    <section id="performance">
        <h2>Performance</h2>
        
        <h3>10D Gaussian Mixture Model Benchmark</h3>
        <p>PARIS demonstrates exceptional efficiency in high-dimensional, multi-modal scenarios. In a challenging 10D GMM with 10 equally weighted modes:</p>
        
        <table style="margin: 20px auto; border-collapse: collapse; width: 100%; max-width: 600px;">
            <tr style="background-color: #f5f5f5;">
                <th style="border: 1px solid #ddd; padding: 12px; text-align: left;"><strong>Method</strong></th>
                <th style="border: 1px solid #ddd; padding: 12px; text-align: center;"><strong>Sample Number</strong></th>
                <th style="border: 1px solid #ddd; padding: 12px; text-align: center;"><strong>Total Calls</strong></th>
                <th style="border: 1px solid #ddd; padding: 12px; text-align: center;"><strong>Log Evidence</strong></th>
            </tr>
            <tr style="background-color: #e8f4f8; font-weight: bold;">
                <td style="border: 1px solid #ddd; padding: 12px;">PARIS</td>
                <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">145,420</td>
                <td style="border: 1px solid #ddd; padding: 12px; text-align: center;"><strong>150,050</strong></td>
                <td style="border: 1px solid #ddd; padding: 12px; text-align: center;"><strong>2.30</strong></td>
            </tr>
            <tr>
                <td style="border: 1px solid #ddd; padding: 12px;">Dynesty</td>
                <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">145,423</td>
                <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">8,587,847</td>
                <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">2.30</td>
            </tr>
            <tr>
                <td style="border: 1px solid #ddd; padding: 12px;">PTMCMC</td>
                <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">145,400</td>
                <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">822,352</td>
                <td style="border: 1px solid #ddd; padding: 12px; text-align: center;">1.91</td>
            </tr>
        </table>

        <div style="text-align: center; margin: 30px 0;">
            <img src="images/GMM10D10M.png" alt="10D GMM Performance Comparison" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px;">
        </div>

        <div class="feature-card">
            <h3>üèÜ Key Results</h3>
            <ul>
                <li><strong>üöÄ 57√ó fewer likelihood evaluations</strong> than Dynesty (Dynamic Nested Sampling)</li>
                <li><strong>‚ö° 5.5√ó fewer likelihood evaluations</strong> than PTMCMC</li>
                <li><strong>üéØ Accurate evidence estimation</strong> (2.30 vs theoretical 2.30)</li>
                <li><strong>‚öñÔ∏è Consistent mode recovery</strong> across all dimensions</li>
                <li><strong>üìä Robust performance</strong> with N<sub>LHS</sub>=10‚Å¥, N<sub>seed</sub>=100</li>
            </ul>
            <p><em>Figure: 1D marginalized posterior comparison. PARIS (green) closely matches the analytical target distribution (grey), while competitors show mode recovery bias. The target distribution's uniform-like appearance in 1D projections results from LHS-based placement of GMM component centers ensuring maximal separation. PARIS achieves this accuracy with dramatically fewer likelihood evaluations.</em></p>
        </div>
    </section>

    <section id="examples">
        <h2>Examples</h2>
        <p>Check out the example files in the repository:</p>
        <ul>
            <li><a href="https://github.com/mx-Liu123/parismc/blob/main/examples/basic_example.py">Basic Example</a></li>
            <li><a href="https://github.com/mx-Liu123/parismc/blob/main/examples/multimodal_example.py">Multimodal Example</a></li>
        </ul>
    </section>

    <section id="key-hyperparameters">
        <h2>Key Hyperparameters</h2>
        <ul>
            <li><strong>merge_confidence (p)</strong>: Controls the merge radius (Mahalanobis threshold) between seeds. <em>p = 0.9</em> is generally fine for most problems.</li>
            <li><strong>alpha</strong>: Number of most recent samples used for importance weighting (truncation window). A conservative and safe choice is <em>alpha = 10000</em>. <em>(Called <code>latest_prob_index</code> in older docs.)</em></li>
        </ul>
    </section>

    <section id="api-reference">
        <h2>API Reference</h2>
        
        <h3>SamplerConfig</h3>
        <p>Configuration dataclass containing all hyperparameters for the sampler.</p>
        
        <div class="code-block">from parismc import SamplerConfig

config = SamplerConfig(
    merge_confidence=0.9,
    alpha=1000,
    trail_size=1000,
    boundary_limiting=True,
    use_beta=True,
    integral_num=100000,
    gamma=100,
    exclude_scale_z=np.inf,
    use_pool=False,
    n_pool=10
)</div>

        <h4>Parameters</h4>
        <ul>
            <li><strong>merge_confidence</strong> (float, default=0.9): Coverage probability mapped to Mahalanobis merge radius <em>R<sub>m</sub></em> by <code>R_m = find_sigma_level(ndim, p)</code>. Higher p ‚Üí larger R<sub>m</sub> (more permissive). Edge cases: p=0 ‚áí R<sub>m</sub>=0; p‚Üí1 ‚áí R<sub>m</sub>‚Üí‚àû. When comparing two processes, compute distances using each process's own covariance and compare the smaller distance to R<sub>m</sub>.</li>
            <li><strong>alpha</strong> (int, default=1000): <em>Paper: Œ±</em> - Only the latest Œ± samples are used for importance weight calculations (truncation for temporal locality).</li>
            <li><strong>trail_size</strong> (int, default=1000): <em>Paper: MaxResample</em> - Maximum number of attempts after rejecting an invalid sample during boundary-constrained sampling.</li>
            <li><strong>boundary_limiting</strong> (bool, default=True): <em>Paper: Always True</em> - Enable boundary constraint handling for [0,1]<sup>d</sup> unit cube. Essential for proper sampling within parameter bounds.</li>
            <li><strong>use_beta</strong> (bool, default=True): <em>Paper: Beta correction</em> - Apply beta correction for boundary truncation effects when >10% samples fall outside [0,1]<sup>d</sup>. Part of advanced covariance estimation.</li>
            <li><strong>integral_num</strong> (int, default=100000): <em>Paper: 10<sup>5</sup> MC samples</em> - Number of Monte Carlo samples used for beta coefficient estimation in boundary correction. Higher values improve accuracy.</li>
            <li><strong>gamma</strong> (int, default=100): <em>Paper: Œ≥</em> - Frequency of covariance matrix updates (in iterations). In practice, proposal covariance Œ£<sub>T</sub><sup>(j)</sup> updated every Œ≥ iterations rather than every iteration.</li>
            <li><strong>exclude_scale_z</strong> (float, default=inf): <em>Not in paper experiments</em> - Exclusion scale for outlier detection. Samples beyond this many standard deviations excluded from covariance updates. Set to ‚àû (disabled) in paper.</li>
            <li><strong>use_pool</strong> (bool, default=False): <em>Computational</em> - Enable multiprocessing for parallel likelihood evaluations. Not part of algorithmic specification.</li>
            <li><strong>n_pool</strong> (int, default=10): <em>Computational</em> - Number of parallel processes when multiprocessing enabled. Implementation detail for computational efficiency.</li>
        </ul>

        <h3>Sampler</h3>
        <p>Main sampling class implementing adaptive importance sampling with clustering.</p>
        
        <div class="code-block">sampler = Sampler(
    ndim=2,
    n_seed=5,
    log_density_func=log_likelihood,
    init_cov_list=[np.eye(2) * 0.1] * 5,
    prior_transform=None,
    config=config
)</div>

        <h4>Constructor Parameters</h4>
        <ul>
            <li><strong>ndim</strong> (int): Dimensionality of the parameter space.</li>
            <li><strong>n_seed</strong> (int): Number of initial seed points (processes) for parallel exploration.</li>
            <li><strong>log_density_func</strong> (callable): Function computing log densities. Must accept numpy array of shape (N, ndim) and return array of shape (N,).</li>
            <li><strong>init_cov_list</strong> (list): List of initial covariance matrices (ndim √ó ndim) for each process.</li>
            <li><strong>prior_transform</strong> (callable, optional): Function transforming from unit cube [0,1]^d to parameter space. If None, assumes unit cube sampling.</li>
            <li><strong>config</strong> (SamplerConfig, optional): Configuration object. Uses defaults if not provided.</li>
        </ul>

        <h4>Key Methods</h4>
        
        <h5>prepare_lhs_samples(lhs_num, batch_size)</h5>
        <p>Initialize the sampler with Latin Hypercube Sampling.</p>
        <ul>
            <li><strong>lhs_num</strong> (int): Total number of initial samples to generate.</li>
            <li><strong>batch_size</strong> (int): Size of batches for likelihood evaluation.</li>
        </ul>

        <h5>run_sampling(num_iterations, savepath, print_iter=1)</h5>
        <p>Execute the main sampling loop.</p>
        <ul>
            <li><strong>num_iterations</strong> (int): Number of sampling iterations to run.</li>
            <li><strong>savepath</strong> (str): Directory path for saving intermediate results.</li>
            <li><strong>print_iter</strong> (int, optional): Print progress every N iterations.</li>
        </ul>

        <h5>get_samples_with_weights(flatten=False)</h5>
        <p>Retrieve samples and their importance weights.</p>
        <ul>
            <li><strong>flatten</strong> (bool): If True, return flattened arrays. If False, return lists of arrays per process.</li>
            <li><strong>Returns</strong>: Tuple of (samples, weights) - either as lists or flattened arrays.</li>
        </ul>

        <h5>save_state(filename=None) / load_state(filename)</h5>
        <p>Save/load sampler state for resuming interrupted runs.</p>
        <ul>
            <li><strong>filename</strong> (str): Path to save/load state file.</li>
        </ul>

        <h3>Utility Functions</h3>
        
        <h4>find_sigma_level(ndim, prob)</h4>
        <p>Compute sigma level for a given confidence probability in ndim space.</p>
        <ul>
            <li><strong>ndim</strong> (int): Dimensionality of the space.</li>
            <li><strong>prob</strong> (float): Confidence probability (0-1).</li>
            <li><strong>Returns</strong>: Sigma level corresponding to the probability.</li>
        </ul>

        <h4>oracle_approximating_shrinkage()</h4>
        <p>Covariance regularization using oracle approximating shrinkage estimator.</p>

        <h4>Weighting Functions</h4>
        <ul>
            <li><strong>weighting_seeds_manypoint()</strong>: Compute importance weights for multiple points against multiple proposal means.</li>
            <li><strong>weighting_seeds_manycov()</strong>: Compute weights using multiple covariance matrices.</li>
            <li><strong>weighting_seeds_onepoint_with_onemean()</strong>: Compute weights for one-to-one point-mean correspondence.</li>
        </ul>
    </section>

    <section id="tuning-tips">
        <h2>Tuning Tips</h2>
        <ul>
            <li><strong>n_lhs</strong> (<code>lhs_num</code> in API): Number of LHS points covering the prior for a global search of good start points. Estimate from the relative size of a typical mode vs prior region. If a mode occupies fraction f of the prior volume, pick <code>lhs_num ‚â≥ c/f</code> (with <code>c‚âà50‚Äì200</code>) to get multiple hits per mode; 10¬≥‚Äì10‚Åµ is common by dimension.</li>
            <li><strong>n_seed</strong>: Depends on a conservative estimate of total mode count. Recommend <code>n_seed = 10 √ó</code> expected modes to avoid missing weaker modes.</li>
            <li><strong>init_cov_list</strong>: Initial covariance per process. Use a conservative small estimate of mode size, or the inverse Fisher matrix if available. On a unit cube, a reasonable start is <code>diag((0.05‚Äì0.1)^2)</code> per dimension, adjusted per scale.</li>
            <li><strong>Less sensitive</strong>: <code>alpha</code> and <code>merge_confidence</code> usually work well with defaults. Use <code>alpha=10000</code> for a safe window and <code>merge_confidence p=0.9</code> for general use.</li>
        </ul>
    </section>

    <section>
        <h2>Requirements</h2>
        <ul>
            <li>Python >= 3.8</li>
            <li>NumPy >= 1.20.0</li>
            <li>SciPy >= 1.7.0</li>
            <li>scikit-learn >= 1.0.0</li>
            <li>smt >= 2.0.0</li>
            <li>tqdm >= 4.62.0</li>
        </ul>
    </section>

    <section>
        <h2>Citation</h2>
        <div class="code-block">@software{parismc,
  title={Parallel Adaptive Reweighting Importance Sampling (PARIS)},
  author={Miaoxin Liu, Alvin J. K. Chua},
  year={2025},
  url={https://github.com/mx-Liu123/parismc}
}</div>
    </section>

    <footer style="text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid #eee;">
        <p>MIT License | <a href="https://github.com/mx-Liu123/parismc">GitHub Repository</a></p>
    </footer>
</body>

</html>
